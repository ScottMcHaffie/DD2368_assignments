{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKbhGX4qhBbl"
      },
      "source": [
        "# Assignment Module 3: Quantum Neural Networks (QNN)\n",
        "\n",
        "This assignment focuses on quantum neural networks (QNNs) for binary classification using a PennyLane + PyTorch implementation of a fully quantum classifier. To complete the assignment, you will implement missing code (marked **YOUR CODE HERE**) and answer a set of short theoretical questions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY3kVTFohLMY"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "* Look at the notebooks and slides on **Quantum Neural Networks (QNNs)** and **PennyLane-PyTorch integration** provided in this module.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHaQe2XQhQv3"
      },
      "source": [
        "## Use of generative AI tools\n",
        "\n",
        "You may use AI-based tools (e.g., ChatGPT, GitHub Copilot, Claude, Gemini, DeepSeek, ...) for brainstorming, refactoring, coding assistance, plotting, or editing.\n",
        "\n",
        "This is allowed with disclosure. LLMs are a great tool, but you have to make sure to grasp the contents of the course!\n",
        "\n",
        "**Make sure to fill in the mandatory AI-disclosure in the notebook before submitting!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZn4B3z4hWlk"
      },
      "source": [
        "## Preparatory code\n",
        "\n",
        "Run this cell to import the modules we need.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RlbuFoiMg48D"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pennylane version:  0.42.3\n",
            "torch version:      2.9.1+cpu\n",
            "Torch device: cpu\n",
            "Imports OK\n"
          ]
        }
      ],
      "source": [
        "# Reproducibility\n",
        "SEED = 123\n",
        "\n",
        "# Imports\n",
        "import math, sys, os, json, pathlib\n",
        "import numpy as np\n",
        "np.random.seed(SEED)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        "    classification_report,\n",
        ")\n",
        "\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as pnp  # optional\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "print(\"pennylane version: \", qml.__version__)\n",
        "print(\"torch version:     \", torch.__version__)\n",
        "\n",
        "\n",
        "# no need to use GPU for this assignment\n",
        "device_torch = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Torch device:\", device_torch)\n",
        "\n",
        "print(\"Imports OK\")\n",
        "\n",
        "FP_TOL = 0.05  # floating point tolerance for automated tests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTZolvBphavt"
      },
      "source": [
        "# Task 0: Loading the dataset\n",
        "\n",
        "In this exercise, we will use the digits dataset from scikit-learn, but we restrict ourselves to a binary classification problem (two digits only, e.g., digits 0 and 1).\n",
        "\n",
        "Before designing the QNN, we must first:\n",
        "\n",
        "1. Load the dataset and select only two classes (0 and 1).  \n",
        "2. Split the data points into a training, validation and test sets.\n",
        "3. Apply a scaling to the input features using `StandardScaler()`, which ensures that the features have zero mean and unit variance.\n",
        "\n",
        "Remember to fit the scaler on the training data, and use the same fitted scaler on both the training, validation and test datasets!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_GdPtKZOhlZB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes: (252, 64) (54, 64) (54, 64)  (class 1 count): 127  (class 0 count): 125\n",
            "Test mean/std: -0.027555 0.864232\n"
          ]
        }
      ],
      "source": [
        "# Data loading and preprocessing\n",
        "# TODO: Load digits dataset, pick two classes (0 and 1), train/test split, and scale features.\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "# Load full digits dataset\n",
        "digits = load_digits()\n",
        "\n",
        "# All data and labels. Each x is the pixel value of the digit between 0 and 16 \n",
        "# in a 8 x 8 image, and each y is the associated target value\n",
        "X_all = digits.data      # shape: (n_samples, n_features)\n",
        "y_all = digits.target    # shape: (n_samples,)\n",
        "\n",
        "# -----YOUR CODE HERE-----\n",
        "# 1. Create a boolean mask that selects only digits 0 and 1.\n",
        "mask = np.zeros_like(y_all, dtype=bool)\n",
        "mask = (y_all == 0) | (y_all == 1)\n",
        "\n",
        "# 2. Apply the mask to X_all and y_all to get a binary dataset.\n",
        "X = X_all[mask][:]\n",
        "y01 = y_all[mask]\n",
        "\n",
        "# 3. Split the dataset in 70% training, 15% validation and 15% test.\n",
        "# Remember to use the seed random_state=SEED, and to stratify according to y01 and y_temp.\n",
        "X_tr, X_temp, y_tr, y_temp = train_test_split(X, \n",
        "                                y01, \n",
        "                                test_size=0.3, \n",
        "                                random_state=SEED, \n",
        "                                stratify=y01\n",
        "                                )\n",
        "X_val, X_te, y_val, y_te = train_test_split(X_temp, \n",
        "                                y_temp, \n",
        "                                test_size=0.5, \n",
        "                                random_state=SEED,\n",
        "                                stratify=y_temp\n",
        "                                )\n",
        "\n",
        "# print (X.shape)\n",
        "# print (len(X))\n",
        "# print (X_tr.shape)\n",
        "# print (X_temp.shape)\n",
        "# print (X_val.shape, X_te.shape)\n",
        "\n",
        "# 4. Make mean 0 and standard deviation 1 using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_tr = scaler.fit_transform(X=X_tr)\n",
        "X_val = scaler.transform(X=X_val)\n",
        "X_te = scaler.transform(X=X_te)\n",
        "# ---YOUR CODE ENDS HERE---\n",
        "\n",
        "# print (X_tr)\n",
        "\n",
        "print(\"Shapes:\", X_tr.shape, X_val.shape, X_te.shape,\n",
        "        \" (class 1 count):\", (y_tr==1).sum(),\n",
        "        \" (class 0 count):\", (y_tr==0).sum())\n",
        "\n",
        "print(\"Test mean/std:\", X_te.mean().round(6), X_te.std().round(6))\n",
        "\n",
        "assert len(X_tr) + len(X_val) + len(X_te) == len(X)\n",
        "assert set(np.unique(y_tr)).issubset({0,1})\n",
        "assert X_tr.shape == (252, 64)\n",
        "assert X_val.shape == (54, 64)\n",
        "assert X_te.shape == (54, 64)\n",
        "assert (y_tr==1).sum() == 127\n",
        "assert (y_tr==0).sum() == 125\n",
        "assert np.isclose(X_tr.mean(), 0.0, FP_TOL, FP_TOL)\n",
        "assert np.isclose(X_te.mean(), -0.033565, FP_TOL, FP_TOL)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 0.1 — Short answer\n",
        "Why is feature scaling important when we use angle-embedding (e.g. RY rotations) to encode classical data into a QNN?  \n",
        "Write a concise justification (2–4 sentences).\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: Feature scaling is important when we use angle-embedding because angle-embedding directly converts each data point to a rotation angle. If the data is not normalized then angle wrapping can occur for data values that are larger than $2\\pi$. Angle wrapping occurs for all data points $x$ that satisfy $x = x + 2\\pi k$, where $k$ is an integer, and maps unique inputs to the same quantum state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 0.2 — Short answer\n",
        "What does the parameter stratify do? (1-2 sentences).\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: The stratify parameter in the ```sklearn.model_selection.train_test_split``` function allows ```train_test_split``` to split a dataset into train and test sets while preserving the ratio of classes specified in the dataset that the keyword ```stratify``` is assigned to. Without using the stratify parameter the test and train datasets are not guaranteed to hold a percentage of each class that is equal to the dataset that was split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yRMih0Hh9xg"
      },
      "source": [
        "# Task 1: Dimensionality reduction using PCA\n",
        "\n",
        "For angle-based maps we often set the number of qubits equal (or proportional) to the feature dimension. Unfortunately, the available qubit count is often smaller than the dimensionality of the data.\n",
        "\n",
        "Here, we use PCA to reduce the data dimensionality to `n_qubits` components, so that we can encode the data on a small QNN.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Choose `n_qubits` (e.g., 4).  \n",
        "2. Fit `PCA(n_components=n_qubits)` on the training data and transform both training, validation and test sets.  \n",
        "3. Print shapes to confirm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uNCIAp6hiDWl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced shapes: (252, 4) (54, 4) (54, 4)\n"
          ]
        }
      ],
      "source": [
        "# PCA reduction to match qubits\n",
        "# -----YOUR CODE HERE-----\n",
        "n_qubits = 4\n",
        "pca = PCA(n_components=n_qubits)\n",
        "Xtr_red = pca.fit_transform(X=X_tr)\n",
        "Xval_red = pca.transform(X=X_val)\n",
        "Xte_red = pca.transform(X=X_te)\n",
        "# ---YOUR CODE ENDS HERE---\n",
        "print(\"Reduced shapes:\", Xtr_red.shape, Xval_red.shape, Xte_red.shape)\n",
        "assert Xtr_red.shape == (252, 4)\n",
        "assert Xval_red.shape == (54, 4)\n",
        "assert Xte_red.shape == (54, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 1.1 — Short answer\n",
        "Explain briefly (2–3 sentences) why PCA is useful in this QNN setting, and what we are trading off by compressing the features.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIGmqSQliHjU"
      },
      "source": [
        "**Answer**: PCA is useful for this QNN because it allows us to train our QNN fewer qubits, which is expected to train faster and less computationally expensive. Additionally, we expect PCA to work well for the digit classification problem because there are many pixels that will not be used for the drawing of the numbers 0 and 1 (meaning the variance of that feature is 0 and it will be discarded), and we expect most of the information to be represented in only a few pixels. The tradeoff to compressing the features is that we lose some detail of the original dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8TYoBGFiNpL"
      },
      "source": [
        "# Task 2: Implement a QNN feature map and variational ansatz\n",
        "\n",
        "We now implement the quantum circuit building blocks of our QNN:\n",
        "\n",
        "1. A **feature map** that encodes each PCA-reduced input into `n_qubits` using angle-embedding.  \n",
        "2. A **variational ansatz** with trainable parameters $\\theta$, which will be learned during training.\n",
        "\n",
        "We will then use these building blocks inside a PennyLane QNode.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7J8ZPki3iU8J"
      },
      "outputs": [],
      "source": [
        "# Task 2: QNN building blocks (feature map + ansatz)\n",
        "\n",
        "# 1. Define a device with n_qubits wires in analytic mode (shots=None)\n",
        "# -----YOUR CODE HERE-----\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits, shots=None)\n",
        "# ---YOUR CODE ENDS HERE---\n",
        "\n",
        "def feature_map_qnn(x, scale=1.0, entangle=True):\n",
        "    \"\"\"\n",
        "    Simple angle-embedding feature map with optional entangling layer.\n",
        "\n",
        "    x: 1D array-like of length n_qubits (PCA-reduced input)\n",
        "    scale: rescaling factor for angles\n",
        "    entangle: if True, apply a CZ ring after single-qubit rotations\n",
        "    \"\"\"\n",
        "    # 1. Apply AngleEmbedding with rotation=\"Y\" and the given scale\n",
        "    qml.AngleEmbedding(features=x*scale, wires=range(n_qubits), rotation='Y')\n",
        "    \n",
        "    # 2. If entangle=True, apply a ring of CZ or CNOT gates\n",
        "    if entangle:\n",
        "        for i in range(n_qubits):\n",
        "            qml.CZ(wires=[i, (i + 1) % n_qubits])\n",
        "    \n",
        "    return None\n",
        "\n",
        "def variational_ansatz(theta):\n",
        "    \"\"\"\n",
        "    Variational circuit with trainable parameters theta.\n",
        "    \n",
        "    theta: parameters with shape (number of layers, number of wires).\n",
        "\n",
        "    Structure:\n",
        "    - For each layer:\n",
        "        - Apply RY rotations on all qubits\n",
        "        - Apply a ring of CNOTs to entangle the qubits\n",
        "    \"\"\"\n",
        "    n_layers, n_wires = theta.shape\n",
        "    # -----YOUR CODE HERE-----\n",
        "    for layer in range(n_layers):\n",
        "        # Apply RY rotations to all qubits\n",
        "        for qubit in range(n_wires):\n",
        "            qml.RY(phi=theta[layer, qubit], wires=qubit)\n",
        "        \n",
        "        # Apply ring of CNOTs to entangle qubits\n",
        "        for qubit in range(n_wires):\n",
        "            qml.CNOT(wires=[qubit, (qubit + 1) % n_wires])\n",
        "    # ---YOUR CODE ENDS HERE---\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 2.1 — Short answer\n",
        "Explain in a few sentences how the feature map and the variational ansatz play different roles in a QNN.  \n",
        "Relate each of them to analogous components in a classical neural network.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: The feature map plays the role of transforming classical data into a quantum state using a transformation with fixed parameters and is analogous to the feature vectors that act as input into a classical neural network. The variational ansatz is a circuit that depends on **trainable parameters** and it's the depth and structure of this ansatz that control the expressivity and trainability of the QNN. The variational ansatz is analogous to the hidden layers of a classical neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 2.2 — Reflection\n",
        "Why does the absence of entangling gates in a quantum feature map or variational ansatz (or any quantum circuit in general) guarantee that the the circuit's output state can be efficiently simulated classically given a classical input such as $|0\\rangle$? (2-5 sentances)\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: Without applying any entangling states the circuit's output state can be efficiently simulated classically because each qubit state is evolving independently. Now only single qubit states must be stored which can easily be done with classical bits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 2.3 — Follow up\n",
        "Given this fact, can we expect any quantum advantage from a circuit composed only of single-qubit gates in a feature map or variational ansatz? Explain why or why not.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: No, we don't expect a quantum advantage because this type of circuit could be easily implemented classically. Additionally, one speedup in quantum computing comes stems from the entanglement of different qubits which allows for the computation of many entangled states to be exectued simultaneously."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALTWnbWRiosT"
      },
      "source": [
        "# Task 3: Define the QNode and PyTorch QNN classifier\n",
        "\n",
        "We now combine the feature map and ansatz into a QNode that returns a single expectation value, and then wrap it in a PyTorch module.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Define a QNode `qnn_circuit(x, theta)`:\n",
        "   - encodes one input sample `x` using `feature_map_qnn`,\n",
        "   - applies `variational_ansatz(theta)`. Try first with scale 1 and with entangling.\n",
        "   - Return the expectation value of `PauliZ` on qubit 0. (Note that measuring PauliZ ensures that the expectation value is in [-1, 1]). Hint: Use the pennylane function `qml.expval`.\n",
        "\n",
        "2. Define a `QNNClassifier(nn.Module)` that:\n",
        "   - stores `theta` as an `nn.Parameter`,\n",
        "   - in `forward`, loops over a mini-batch of inputs,\n",
        "   - calls the QNode for each sample,\n",
        "   - maps the expectation values from [-1, 1] to probabilities in [0, 1],\n",
        "   - returns a tensor of shape `(batch_size, 1)`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 3.1 — Short answer\n",
        "Why is the expectation value of a Pauli operator (e.g. Z on one qubit) a natural scalar output for a QNN used for **binary classification**?  \n",
        "How do we convert it into a probability in [0, 1] in a good way?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: The expectation value of a Pauli operator is a natural scalar output for binary classification because it has expectation values [-1, 1] which is already a relatively common choice of labels in binary classification problems and provides symmetry around 0 for the model. \n",
        "\n",
        "To convert the Pauli Z operator expectation values, $\\langle Z \\rangle = $ [-1, 1], to a probability $p$ of [0, 1] we can apply the following equation:\n",
        "\\begin{equation*}\n",
        "    p = \\frac{1 + \\langle Z \\rangle}{2} .\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "50IkgM-QivyN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QNN circuit output (expectation value): -0.5613872548613136\n",
            "QNNClassifier()\n"
          ]
        }
      ],
      "source": [
        "# Task 3: QNode and QNN classifier with PyTorch\n",
        "\n",
        "# Decide a theta shape, e.g. for L layers and n_qubits:\n",
        "# theta_shape = (n_layers, n_qubits)\n",
        "# You may choose a different layout if you like.\n",
        "n_layers = 2\n",
        "theta_shape = (n_layers, n_qubits)\n",
        "\n",
        "@qml.qnode(dev, interface=\"torch\", diff_method=\"parameter-shift\")\n",
        "def qnn_circuit(x, theta):\n",
        "    \"\"\"\n",
        "    Quantum node for a single input sample.\n",
        "\n",
        "    x: 1D tensor with n_qubits features\n",
        "    theta: trainable parameters for the ansatz\n",
        "    \"\"\"\n",
        "    # -----YOUR CODE HERE-----\n",
        "    feature_map_qnn(x, scale = 1.0, entangle = True)\n",
        "    variational_ansatz(theta)\n",
        "    return qml.expval(qml.PauliZ(0))\n",
        "    # ---YOUR CODE ENDS HERE---\n",
        "    \n",
        "def test_qnn_circuit():\n",
        "    x_sample = torch.tensor([0.1, -0.2, 0.3, 0.4], dtype=torch.float32)\n",
        "    theta_sample = torch.ones(theta_shape, dtype=torch.float32)\n",
        "    ev = qnn_circuit(x_sample, theta_sample)\n",
        "    print(\"QNN circuit output (expectation value):\", ev.item())\n",
        "    assert np.isclose(ev.item(), -0.5613872, FP_TOL, FP_TOL)\n",
        "\n",
        "test_qnn_circuit()\n",
        "\n",
        "class QNNClassifier(nn.Module):\n",
        "    def __init__(self, theta_shape):\n",
        "        super().__init__()\n",
        "        # Initialize trainable parameters theta as a PyTorch Parameter\n",
        "        self.theta = nn.Parameter(torch.randn(theta_shape) * 0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: tensor of shape (batch_size, n_qubits)\n",
        "        returns: probabilities in [0,1] of shape (batch_size, 1)\n",
        "        \"\"\"\n",
        "        # -----YOUR CODE HERE-----\n",
        "        # 1. Ensure x has a batch dimension\n",
        "        if x.ndim == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "\n",
        "        # 2. For each sample in the batch, call qnn_circuit(sample, self.theta)\n",
        "        exp = []\n",
        "        for index in range(x.shape[0]):\n",
        "            sample = x[index]\n",
        "            ev = qnn_circuit(sample, self.theta)  # returns differentiable torch tensor\n",
        "            exp.append(ev)\n",
        "\n",
        "        # 3. Stack expectation values WITHOUT breaking autograd\n",
        "        exp = torch.stack(exp)   # <-- FIXED\n",
        "\n",
        "        # 4. Map from [-1,1] to [0,1]\n",
        "        p = (exp + 1.0) / 2.0\n",
        "\n",
        "        # 5. Return p with shape (batch_size, 1)\n",
        "        return p.unsqueeze(1).to(device_torch)\n",
        "        # ---YOUR CODE ENDS HERE---\n",
        "\n",
        "# Instantiate model and move to Torch device\n",
        "model = QNNClassifier(theta_shape).to(device_torch)\n",
        "print(model)\n",
        "\n",
        "def test_qnn_circuit():\n",
        "    test_batch1 = torch.tensor([[0.1, -0.2, 0.3, 0.4],\n",
        "                                [0.5, 0.6, -0.7, 0.8]], \n",
        "                                dtype=torch.float32)\n",
        "    \n",
        "    test_batch2 = torch.tensor([0.0, 0.0, 0.0, 0.0], dtype=torch.float32)\n",
        "        \n",
        "    assert model.forward(test_batch1).shape == (2, 1)\n",
        "    assert model.forward(test_batch2).shape == (1, 1)\n",
        "    \n",
        "    #Assert that the output is a probability\n",
        "    p = model.forward(test_batch2).item()\n",
        "    assert 0.0 <= p <= 1.0\n",
        "\n",
        "test_qnn_circuit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_G2PONdi9vw"
      },
      "source": [
        "# Task 4: Training the QNN\n",
        "\n",
        "We now train the QNN classifier on the PCA-reduced dataset.\n",
        "\n",
        "We will use:\n",
        "\n",
        "* `nn.BCELoss` as the loss function (binary cross-entropy on probabilities),  \n",
        "* `Adam` as the optimizer,  \n",
        "* a modest number of epochs (e.g. 15–30).\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Wrap the training data in a `DataLoader`. Shuffle the data for the training loader but not the validation loader.\n",
        "2. Implement the standard training loop:\n",
        "   - zero gradients,\n",
        "   - forward pass,\n",
        "   - compute loss,\n",
        "   - backpropagate,\n",
        "   - optimizer step.  \n",
        "3. Track and print the training loss per epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tkqrqtI4iK9S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8 2 2\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x000001E986790710>\n"
          ]
        }
      ],
      "source": [
        "# Create DataLoaders for training and test sets using PCA-reduced features\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = TensorDataset(\n",
        "    torch.tensor(Xtr_red, dtype=torch.double),\n",
        "    torch.tensor(y_tr, dtype=torch.double).unsqueeze(1),\n",
        ")\n",
        "val_ds = TensorDataset(\n",
        "    torch.tensor(Xval_red, dtype=torch.double),\n",
        "    torch.tensor(y_val, dtype=torch.double).unsqueeze(1),\n",
        ")\n",
        "test_ds = TensorDataset(\n",
        "    torch.tensor(Xte_red, dtype=torch.double),\n",
        "    torch.tensor(y_te, dtype=torch.double).unsqueeze(1),\n",
        ")\n",
        "# -----YOUR CODE HERE-----\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "# ---YOUR CODE ENDS HERE---\n",
        "\n",
        "print(len(train_loader), len(val_loader), len(test_loader))\n",
        "assert len(train_loader) == 8\n",
        "assert len(val_loader) == 2\n",
        "assert len(test_loader) == 2\n",
        "print(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pD7plbHYh67g"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01/20 - loss: 0.6593 - val_loss: 0.6697\n",
            "Epoch 02/20 - loss: 0.6215 - val_loss: 0.6237\n",
            "Epoch 03/20 - loss: 0.5970 - val_loss: 0.5976\n",
            "Epoch 04/20 - loss: 0.5837 - val_loss: 0.5824\n",
            "Epoch 05/20 - loss: 0.5759 - val_loss: 0.5731\n",
            "Epoch 06/20 - loss: 0.5723 - val_loss: 0.5691\n",
            "Epoch 07/20 - loss: 0.5696 - val_loss: 0.5647\n",
            "Epoch 08/20 - loss: 0.5669 - val_loss: 0.5638\n",
            "Epoch 09/20 - loss: 0.5654 - val_loss: 0.5666\n",
            "Epoch 10/20 - loss: 0.5650 - val_loss: 0.5669\n",
            "Epoch 11/20 - loss: 0.5637 - val_loss: 0.5644\n",
            "Epoch 12/20 - loss: 0.5634 - val_loss: 0.5620\n",
            "Epoch 13/20 - loss: 0.5626 - val_loss: 0.5595\n",
            "Epoch 14/20 - loss: 0.5611 - val_loss: 0.5567\n",
            "Epoch 15/20 - loss: 0.5597 - val_loss: 0.5549\n",
            "Epoch 16/20 - loss: 0.5587 - val_loss: 0.5526\n",
            "Epoch 17/20 - loss: 0.5567 - val_loss: 0.5510\n",
            "Epoch 18/20 - loss: 0.5542 - val_loss: 0.5527\n",
            "Epoch 19/20 - loss: 0.5522 - val_loss: 0.5511\n",
            "Epoch 20/20 - loss: 0.5495 - val_loss: 0.5533\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n",
        "\n",
        "n_epochs = 20\n",
        "\n",
        "history = {\"loss\": [], \"val_loss\": []}\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.to(device_torch)\n",
        "        yb = yb.to(device_torch)\n",
        "\n",
        "        # -----YOUR CODE HERE-----\n",
        "        # 1. Zero grads\n",
        "        optimizer.zero_grad()\n",
        "        # 2. Forward pass\n",
        "        probs = model(xb)\n",
        "        # 3. Compute loss\n",
        "        loss = criterion(probs, yb)\n",
        "        # 4. Backpropagate\n",
        "        loss.backward()\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # ---YOUR CODE ENDS HERE---\n",
        "\n",
        "        epoch_loss += loss.item() * xb.size(0)\n",
        "    epoch_loss /= len(train_loader.dataset)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb = xb.to(device_torch)\n",
        "            yb = yb.to(device_torch)\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            val_loss += loss.item() * xb.size(0)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "\n",
        "    history[\"loss\"].append(epoch_loss)\n",
        "    history[\"val_loss\"].append(val_loss)\n",
        "    print(f\"Epoch {epoch:02d}/{n_epochs} - loss: {epoch_loss:.4f} - val_loss: {val_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5VX4V-XnjK_D"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot training loss\n",
        "\n",
        "plt.figure(figsize=(4, 3))\n",
        "plt.plot(history[\"loss\"], marker='o', label=\"Training\")\n",
        "plt.plot(history[\"val_loss\"], marker='o', label=\"Validation\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Train loss (BCE)\")\n",
        "plt.title(\"QNN training loss\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T04N9kf0jMBw"
      },
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 4.1 — Short answer\n",
        "Comment briefly on the training and validation curves you obtained.\n",
        "Does the loss decrease steadily? Do you see signs of underfitting or overfitting?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: The training loss decreases steadily but the validation loss does not. The validation loss decreases steadily until epoch 17, at which case it increases slightly to epoch 18 and flattens out in epochs 19 and 20. This is a sign of overfitting as the model is still improving its fit on the training data but flattens out on the validation data (the model is struggling to generalize)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOMFzliajR7k"
      },
      "source": [
        "# Task 5: Evaluation on the test set\n",
        "\n",
        "Finally, we evaluate the trained QNN on the **held-out test set**.\n",
        "\n",
        "1. Compute predicted probabilities on the test set.  \n",
        "2. Convert probabilities to class predictions using a 0.5 threshold.  \n",
        "3. Compute and print:\n",
        "   - test accuracy,  \n",
        "   - confusion matrix,  \n",
        "   - classification report (optional).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Mjo4xZ_3jX1m"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.815\n",
            "Confusion matrix [[TN, FP], [FN, TP]]:\n",
            "[[20  7]\n",
            " [ 3 24]]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.74      0.80        27\n",
            "           1       0.77      0.89      0.83        27\n",
            "\n",
            "    accuracy                           0.81        54\n",
            "   macro avg       0.82      0.81      0.81        54\n",
            "weighted avg       0.82      0.81      0.81        54\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluation on test set\n",
        "\n",
        "model.eval()\n",
        "all_probs = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.to(device_torch)\n",
        "        yb = yb.to(device_torch)\n",
        "\n",
        "        probs = model(xb)  # (batch_size, 1)\n",
        "        all_probs.append(probs.cpu())\n",
        "        all_labels.append(yb.cpu())\n",
        "\n",
        "all_probs = torch.cat(all_probs, dim=0)\n",
        "all_labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "# Convert probabilities to class predictions\n",
        "y_pred = (all_probs >= 0.5).int()\n",
        "y_true = all_labels.int()\n",
        "\n",
        "acc = accuracy_score(y_true.numpy(), y_pred.numpy())\n",
        "cm = confusion_matrix(y_true.numpy(), y_pred.numpy())\n",
        "\n",
        "print(f\"Test accuracy: {acc:.3f}\")\n",
        "print(\"Confusion matrix [[TN, FP], [FN, TP]]:\")\n",
        "print(cm)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.show()\n",
        "\n",
        "print(classification_report(y_true.numpy(), y_pred.numpy()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8grB5epjaMx"
      },
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 5.1 — Short answer\n",
        "Does the confusion matrix show any interesting asymmetry (e.g., more false positives than false negatives)?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: Yes, the model produced more false positives (7) than false negatives (3), indicating that the model has a tendency to predict class 1 more often than class 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNltSHpYjeEN"
      },
      "source": [
        "# Bonus: Gradient inspection and barren plateaus\n",
        "\n",
        "In this bonus exercise, you will perform a more detailed gradient inspection on your QNN to connect with the concept of barren plateaus.\n",
        "\n",
        "We will focus on three aspects:\n",
        "\n",
        "1. Gradient norms at initialization for different circuit depths (`n_layers`).  \n",
        "2. Gradient norms during training (how they evolve across epochs).  \n",
        "3. Distribution of per-parameter gradients at a given point in training.\n",
        "\n",
        "\n",
        "You may reuse your QNN architecture from the main tasks (feature map, ansatz, QNode, and `QNNClassifier`), or define a simplified variant dedicated to this analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30UuPJADkctx"
      },
      "source": [
        "## Bonus 0 — Helper: single-batch gradient computation\n",
        "\n",
        "We start by writing a helper function that:\n",
        "\n",
        "- Takes a model, a data loader, and a loss function.  \n",
        "- Grabs one mini-batch from the loader.  \n",
        "- Computes the loss and then the gradient of the loss w.r.t. `model.theta`.  \n",
        "- Returns the L2 norm of that gradient, divided by the square root of the number of parameters (and optionally the gradient tensor itself).\n",
        "\n",
        "We will reuse this helper in the following subtasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqPqS0P_kmpj"
      },
      "outputs": [],
      "source": [
        "# Bonus A.0 — Helper: compute gradient norm for one batch\n",
        "\n",
        "# Normalized gradient norm computation\n",
        "def gradient_norm_for_one_batch(model, data_loader, criterion, device=device_torch):\n",
        "    \"\"\"\n",
        "    Compute the L2 norm of the gradient of the loss w.r.t. model.theta\n",
        "    using a single batch from data_loader.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    # Take the first batch only\n",
        "    xb, yb = next(iter(data_loader))\n",
        "    xb = xb.to(device)\n",
        "    yb = yb.to(device)\n",
        "\n",
        "    # Zero existing gradients\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    preds = model(xb)\n",
        "    loss = criterion(preds, yb)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Extract gradient of theta\n",
        "    grad_theta = model.theta.grad  # tensor with same shape as theta\n",
        "    grad_norm = torch.norm(grad_theta).item() / math.sqrt(grad_theta.numel())\n",
        "\n",
        "    return grad_norm, grad_theta.detach().cpu(), loss.item()\n",
        "\n",
        "# Quick sanity check (optional, only if model is already defined)\n",
        "criterion = nn.BCELoss()\n",
        "g_norm, g_tensor, loss_val = gradient_norm_for_one_batch(model, train_loader, criterion)\n",
        "print(\"Gradient norm (one batch):\", g_norm, \"Loss:\", loss_val)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r_5JmzskrGq"
      },
      "source": [
        "## Bonus 1 — Gradient norms at initialization vs circuit depth\n",
        "\n",
        "In this part, you will study how the gradient norm at random initialization depends on the circuit depth `n_layers`.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Choose a list of depths, e.g. `depth_list = [1, 2, 3]` (you may add more if runtime allows).  \n",
        "2. For each depth:\n",
        "   - Rebuild a fresh QNN (same architecture as in the main task, but with that `n_layers`).  \n",
        "   - Randomly initialize the parameters (the default PyTorch initialization is fine).  \n",
        "   - Compute the gradient norm on a single training batch using `gradient_norm_for_one_batch`.  \n",
        "3. Store the gradient norms and print them (and optionally plot them).  \n",
        "4. Comment on whether deeper circuits tend to show smaller gradient norms at initialization.\n",
        "\n",
        "You can reuse your `QNNClassifier` class, but you will need to make `n_layers` configurable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIsE0jI1kze5"
      },
      "outputs": [],
      "source": [
        "# Bonus A.1 — Gradient norm vs depth at initialization\n",
        "\n",
        "# Make sure you have a way to construct a QNNClassifier with a given n_layers.\n",
        "# For example, you can wrap the model creation in a small factory function.\n",
        "\n",
        "def make_qnn_model(n_layers, n_qubits):\n",
        "    \"\"\"\n",
        "    Factory for QNNClassifier with given depth n_layers and n_qubits.\n",
        "    Adjust theta_shape and ansatz accordingly.\n",
        "    \"\"\"\n",
        "    # Example: theta_shape = (n_layers, n_qubits)\n",
        "    theta_shape = (n_layers, n_qubits)\n",
        "    model = QNNClassifier(theta_shape).to(device_torch)\n",
        "    return model\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "depth_list = [1, 2, 3, 4, 5, 6, 7]  # you can add 4, 5, ... if time allows\n",
        "grad_norms_depth = []\n",
        "\n",
        "for depth in depth_list:\n",
        "# -----YOUR CODE HERE-----\n",
        "\n",
        "# ---YOUR CODE ENDS HERE---\n",
        "\n",
        "# Optional: plot gradient norms vs depth\n",
        "plt.figure(figsize=(4, 3))\n",
        "plt.plot(depth_list, grad_norms_depth, marker=\"o\")\n",
        "plt.xlabel(\"n_layers (circuit depth)\")\n",
        "plt.ylabel(\"Gradient norm at init\")\n",
        "plt.title(\"Gradient norm vs depth at initialization\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yjx5yAkbk2SE"
      },
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question Bonus 1.1 — Interpretation\n",
        "Comment on the gradient norms vs depth that you observed.\n",
        "\n",
        "- Do deeper circuits tend to show smaller gradient norms at random initialization?  \n",
        "- How does this relate qualitatively to the idea of barren plateaus (even in this small setup)?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fcwwuygk-pg"
      },
      "source": [
        "## Bonus 2 — Gradient norm across training epochs\n",
        "\n",
        "We now study how the gradient norm evolves during training for a fixed circuit depth.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Pick one depth (e.g. `n_layers = 2`).  \n",
        "2. Train the QNN for a small number of epochs (e.g. 10) as in the main assignment.  \n",
        "3. At each epoch:\n",
        "   - After the weight update, compute the gradient norm on one batch using `gradient_norm_for_one_batch`.  \n",
        "   - Store this value in a list `grad_norms_epochs`.  \n",
        "4. Plot gradient norm vs epoch, and compare this with the training loss curve.  \n",
        "5. Comment on whether gradients tend to shrink as training progresses.\n",
        "\n",
        "> Hint: You can reuse your training loop, but add a call to `gradient_norm_for_one_batch` at the end of each epoch, using the current model parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwGcF7JDlH8G"
      },
      "outputs": [],
      "source": [
        "# Bonus A.2 — Gradient norm vs epoch for a fixed depth\n",
        "\n",
        "# Choose a depth\n",
        "depth_for_training = 2\n",
        "model_train = make_qnn_model(depth_for_training, n_qubits)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model_train.parameters(), lr=0.02)\n",
        "\n",
        "n_epochs_bonus = 10\n",
        "losses_epochs = []\n",
        "grad_norms_epochs = []\n",
        "\n",
        "for epoch in range(1, n_epochs_bonus + 1):\n",
        "    model_train.train()\n",
        "    epoch_loss = 0.0\n",
        "    # -----YOUR CODE HERE-----\n",
        "\n",
        "    # ---YOUR CODE ENDS HERE---\n",
        "    grad_norms_epochs.append(g_norm)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d}/{n_epochs_bonus} - Loss: {epoch_loss:.4f}, Grad norm: {g_norm:.4e}\")\n",
        "\n",
        "# Plot loss and gradient norm vs epoch\n",
        "fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "ax1.set_xlabel(\"Epoch\")\n",
        "ax1.set_ylabel(\"Train loss (BCE)\", color=\"C0\")\n",
        "ax1.plot(range(1, n_epochs_bonus+1), losses_epochs, marker=\"o\", color=\"C0\", label=\"Loss\")\n",
        "ax1.tick_params(axis=\"y\", labelcolor=\"C0\")\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel(\"Gradient norm\", color=\"C1\")\n",
        "ax2.plot(range(1, n_epochs_bonus+1), grad_norms_epochs, marker=\"s\", color=\"C1\", label=\"Grad norm\")\n",
        "ax2.tick_params(axis=\"y\", labelcolor=\"C1\")\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.title(f\"Depth = {depth_for_training}: Loss and gradient norm vs epoch\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOXamdZPlMAg"
      },
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question Bonus 2.1 — Observation\n",
        "Describe how the gradient norm changes during training: Does it decrease, stay roughly constant, or fluctuate?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question Bonus 2.2 — Interpretation\n",
        "How does this relate to the changes in the training loss?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question Bonus 2.3 — Interpretation\n",
        "In general, if our QNN suffered from a barren plateau, how would that affect the gradient norm?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWswq1DflVCs"
      },
      "source": [
        "## Bonus 3 — Distribution of per-parameter gradients\n",
        "\n",
        "So far, we have focused on the normalized L2 norm of the gradient.  \n",
        "Now, we take a closer look at the distribution of individual gradient components.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Using a model (for some depth), compute the gradient of the loss w.r.t. `model.theta` on one batch.  \n",
        "2. Extract all gradient entries as a 1D array and plot a histogram of their values.  \n",
        "Do step 1 and 2:\n",
        "   - once at initialization, and  \n",
        "   - once after training, and compare the two histograms.\n",
        "\n",
        "This gives a sense of whether gradients are broadly distributed or tend to concentrate tightly around zero.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbWo_z4BlhCM"
      },
      "outputs": [],
      "source": [
        "# Bonus A.3 — Gradient distribution (histogram)\n",
        "\n",
        "def gradient_histogram(model, data_loader, criterion, device=device_torch, title=\"\"):\n",
        "    \"\"\"\n",
        "    Compute gradients on one batch and plot a histogram of gradient values.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    xb, yb = next(iter(data_loader))\n",
        "    xb = xb.to(device)\n",
        "    yb = yb.to(device)\n",
        "    \n",
        "    # -----YOUR CODE HERE-----\n",
        "\n",
        "    # ---YOUR CODE ENDS HERE---\n",
        "\n",
        "    grad_vals = model.theta.grad.detach().cpu().numpy().ravel()\n",
        "\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    plt.hist(grad_vals, bins=40, alpha=0.8)\n",
        "    plt.xlabel(\"Gradient value\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.title(title)\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"{title} — mean grad: {grad_vals.mean():.2e}, std grad: {grad_vals.std():.2e}\")\n",
        "    return grad_vals\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "depth = 2\n",
        "# Example: histogram at initialization for specific depth\n",
        "model_init = make_qnn_model(depth, n_qubits)\n",
        "print(\"Gradient distribution at initialization (depth=2):\")\n",
        "_ = gradient_histogram(model_init, train_loader, criterion, device=device_torch,\n",
        "                       title=\"Gradients at init (depth=2)\")\n",
        "\n",
        "# Example: histogram after training for depth=2 (reuse model_train from Bonus A.2 if available)\n",
        "try:\n",
        "    print(\"Gradient distribution after training (depth=2):\")\n",
        "    _ = gradient_histogram(model_train, train_loader, criterion, device=device_torch,\n",
        "                           title=\"Gradients after training (depth=2)\")\n",
        "except NameError:\n",
        "    print(\"model_train not found. Run Bonus A.2 first or train a model for this analysis.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare the gradient histograms you obtained:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB-ifGkolmQ1"
      },
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question Bonus 3.1 — Observation\n",
        "Are gradients broadly distributed or sharply concentrated near zero?  \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAcEJ8k3jJep"
      },
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question Bonus 3.2 — Short answer\n",
        "Does training make them more or less concentrated?  \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question Bonus 3.3 — Reflection\n",
        "How would a highly concentrated distribution of very small gradients relate to barren plateaus?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feedback to us"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Optional question\n",
        "Was there any part of the tasks where you struggled for some \"unnecessary\" reason? (Errors in the notebook, bad instructions etc.)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your optional answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Disclosure of AI Usage (Mandatory)\n",
        "Fill in this part disclosing any AI usage before submitting the assignment by describing your use of LLMs or other AI-based tools in this assignment.\n",
        "\n",
        "For each task, we ask you to provide information about:\n",
        "- **Tools/models used**.\n",
        "- **Per‑task usage**: for each task, a brief summary of what the tool was used for.\n",
        "- **Prompts/transcripts**: main prompts or a summary of interactions (a link or screenshot is acceptable if long).\n",
        "- **Validation**: how you checked and verified the correctness of AI-generated outputs (tests run, docs consulted, comparisons, plots etc.).\n",
        "\n",
        "Disclosure:\n",
        "- **Task 0**: None.\n",
        "\n",
        "- **Task 1**: None.\n",
        "\n",
        "- **Task 2**: None.\n",
        "\n",
        "- **Task 3**: Used Chat-GPT to resolve the issue of torch tensors being on different devices, and used the fix of ```.to(device_torch)```.\n",
        "\n",
        "- **Task 4**: None.\n",
        "\n",
        "- **Task 5**: None.\n",
        "\n",
        "- **Bonus 1**: [...describe your use...]\n",
        "\n",
        "- **Bonus 2**: [...describe your use...]\n",
        "\n",
        "- **Bonus 3**: [...describe your use...]\n",
        "\n",
        "If you did not use any AI tools for a given exercise, specify this by writing \"None\". If you did not complete the bonus exercises, you can leave those fields empty."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "DD2367",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
