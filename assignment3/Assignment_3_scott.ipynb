{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKbhGX4qhBbl"
      },
      "source": [
        "# Assignment Module 3: Quantum Neural Networks (QNN)\n",
        "\n",
        "This assignment focuses on quantum neural networks (QNNs) for binary classification using a PennyLane + PyTorch implementation of a fully quantum classifier. To complete the assignment, you will implement missing code (marked **YOUR CODE HERE**) and answer a set of short theoretical questions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY3kVTFohLMY"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "* Look at the notebooks and slides on **Quantum Neural Networks (QNNs)** and **PennyLane-PyTorch integration** provided in this module.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHaQe2XQhQv3"
      },
      "source": [
        "## Use of generative AI tools\n",
        "\n",
        "You may use AI-based tools (e.g., ChatGPT, GitHub Copilot, Claude, Gemini, DeepSeek, ...) for brainstorming, refactoring, coding assistance, plotting, or editing.\n",
        "\n",
        "This is allowed with disclosure. LLMs are a great tool, but you have to make sure to grasp the contents of the course!\n",
        "\n",
        "**Make sure to fill in the mandatory AI-disclosure in the notebook before submitting!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZn4B3z4hWlk"
      },
      "source": [
        "## Preparatory code\n",
        "\n",
        "Run this cell to import the modules we need.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "RlbuFoiMg48D"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pennylane version:  0.42.3\n",
            "torch version:      2.9.1+cpu\n",
            "Torch device: cpu\n",
            "Imports OK\n"
          ]
        }
      ],
      "source": [
        "# Reproducibility\n",
        "SEED = 123\n",
        "\n",
        "# Imports\n",
        "import math, sys, os, json, pathlib\n",
        "import numpy as np\n",
        "np.random.seed(SEED)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        "    classification_report,\n",
        ")\n",
        "\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as pnp  # optional\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "print(\"pennylane version: \", qml.__version__)\n",
        "print(\"torch version:     \", torch.__version__)\n",
        "\n",
        "\n",
        "# no need to use GPU for this assignment\n",
        "device_torch = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Torch device:\", device_torch)\n",
        "\n",
        "print(\"Imports OK\")\n",
        "\n",
        "FP_TOL = 0.05  # floating point tolerance for automated tests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTZolvBphavt"
      },
      "source": [
        "# Task 0: Loading the dataset\n",
        "\n",
        "In this exercise, we will use the digits dataset from scikit-learn, but we restrict ourselves to a binary classification problem (two digits only, e.g., digits 0 and 1).\n",
        "\n",
        "Before designing the QNN, we must first:\n",
        "\n",
        "1. Load the dataset and select only two classes (0 and 1).  \n",
        "2. Split the data points into a training, validation and test sets.\n",
        "3. Apply a scaling to the input features using `StandardScaler()`, which ensures that the features have zero mean and unit variance.\n",
        "\n",
        "Remember to fit the scaler on the training data, and use the same fitted scaler on both the training, validation and test datasets!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1797, 64)\n",
            "(1797,)\n",
            "[ 0.  0.  0.  0. 14. 13.  1.  0.  0.  0.  0.  5. 16. 16.  2.  0.  0.  0.\n",
            "  0. 14. 16. 12.  0.  0.  0.  1. 10. 16. 16. 12.  0.  0.  0.  3. 12. 14.\n",
            " 16.  9.  0.  0.  0.  0.  0.  5. 16. 15.  0.  0.  0.  0.  0.  4. 16. 14.\n",
            "  0.  0.  0.  0.  0.  1. 13. 16.  1.  0.]\n"
          ]
        }
      ],
      "source": [
        "# Data loading and preprocessing\n",
        "# TODO: Load digits dataset, pick two classes (0 and 1), train/test split, and scale features.\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "# Load full digits dataset\n",
        "digits = load_digits()\n",
        "\n",
        "# All data and labels\n",
        "X_all = digits.data      # shape: (n_samples, n_features)\n",
        "y_all = digits.target    # shape: (n_samples,)\n",
        "\n",
        "print (X_all.shape)\n",
        "print (y_all.shape)\n",
        "\n",
        "print (X_all[11])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GdPtKZOhlZB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes: (252, 64) (54, 64) (54, 64)  (class 1 count): 127  (class 0 count): 125\n",
            "Test mean/std: -0.027555 0.864232\n"
          ]
        }
      ],
      "source": [
        "# Data loading and preprocessing\n",
        "# TODO: Load digits dataset, pick two classes (0 and 1), train/test split, and scale features.\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "# Load full digits dataset\n",
        "digits = load_digits()\n",
        "\n",
        "# All data and labels. Each x is the pixel value of the digit between 0 and 16 \n",
        "# in a 8 x 8 image, and each y is the associated target value\n",
        "X_all = digits.data      # shape: (n_samples, n_features)\n",
        "y_all = digits.target    # shape: (n_samples,)\n",
        "\n",
        "# -----YOUR CODE HERE-----\n",
        "# 1. Create a boolean mask that selects only digits 0 and 1.\n",
        "mask = np.zeros_like(y_all, dtype=bool)\n",
        "mask = (y_all == 0) | (y_all == 1)\n",
        "\n",
        "# 2. Apply the mask to X_all and y_all to get a binary dataset.\n",
        "X = X_all[mask][:]\n",
        "y01 = y_all[mask]\n",
        "\n",
        "# 3. Split the dataset in 70% training, 15% validation and 15% test.\n",
        "# Remember to use the seed random_state=SEED, and to stratify according to y01 and y_temp.\n",
        "X_tr, X_temp, y_tr, y_temp = train_test_split(X, \n",
        "                                            y01, \n",
        "                                            test_size=0.3, \n",
        "                                            random_state=SEED, \n",
        "                                            stratify=y01\n",
        "                                            )\n",
        "X_val, X_te, y_val, y_te = train_test_split(X_temp, \n",
        "                                            y_temp, \n",
        "                                            test_size=0.5, \n",
        "                                            random_state=SEED,\n",
        "                                            stratify=y_temp\n",
        "                                            )\n",
        "\n",
        "# print (X.shape)\n",
        "# print (len(X))\n",
        "# print (X_tr.shape)\n",
        "# print (X_temp.shape)\n",
        "# print (X_val.shape, X_te.shape)\n",
        "\n",
        "# 4. Make mean 0 and standard deviation 1 using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_tr = scaler.fit_transform(X=X_tr)\n",
        "X_val = scaler.transform(X=X_val)\n",
        "X_te = scaler.transform(X=X_te)\n",
        "# ---YOUR CODE ENDS HERE---\n",
        "\n",
        "# print (X_tr)\n",
        "\n",
        "print(\"Shapes:\", X_tr.shape, X_val.shape, X_te.shape,\n",
        "        \" (class 1 count):\", (y_tr==1).sum(),\n",
        "        \" (class 0 count):\", (y_tr==0).sum())\n",
        "\n",
        "print(\"Test mean/std:\", X_te.mean().round(6), X_te.std().round(6))\n",
        "\n",
        "assert len(X_tr) + len(X_val) + len(X_te) == len(X)\n",
        "assert set(np.unique(y_tr)).issubset({0,1})\n",
        "assert X_tr.shape == (252, 64)\n",
        "assert X_val.shape == (54, 64)\n",
        "assert X_te.shape == (54, 64)\n",
        "assert (y_tr==1).sum() == 127\n",
        "assert (y_tr==0).sum() == 125\n",
        "assert np.isclose(X_tr.mean(), 0.0, FP_TOL, FP_TOL)\n",
        "assert np.isclose(X_te.mean(), -0.033565, FP_TOL, FP_TOL)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 0.1 — Short answer\n",
        "Why is feature scaling important when we use angle-embedding (e.g. RY rotations) to encode classical data into a QNN?  \n",
        "Write a concise justification (2–4 sentences).\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 0.2 — Short answer\n",
        "What does the parameter stratify do? (1-2 sentences).\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yRMih0Hh9xg"
      },
      "source": [
        "# Task 1: Dimensionality reduction using PCA\n",
        "\n",
        "For angle-based maps we often set the number of qubits equal (or proportional) to the feature dimension. Unfortunately, the available qubit count is often smaller than the dimensionality of the data.\n",
        "\n",
        "Here, we use PCA to reduce the data dimensionality to `n_qubits` components, so that we can encode the data on a small QNN.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Choose `n_qubits` (e.g., 4).  \n",
        "2. Fit `PCA(n_components=n_qubits)` on the training data and transform both training, validation and test sets.  \n",
        "3. Print shapes to confirm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNCIAp6hiDWl"
      },
      "outputs": [],
      "source": [
        "# PCA reduction to match qubits\n",
        "# -----YOUR CODE HERE-----\n",
        "n_qubits = 4\n",
        "pca = \n",
        "Xtr_red =\n",
        "Xval_red = \n",
        "Xte_red = \n",
        "# ---YOUR CODE ENDS HERE---\n",
        "print(\"Reduced shapes:\", Xtr_red.shape, Xval_red.shape, Xte_red.shape)\n",
        "assert Xtr_red.shape == (252, 4)\n",
        "assert Xval_red.shape == (54, 4)\n",
        "assert Xte_red.shape == (54, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 1.1 — Short answer\n",
        "Explain briefly (2–3 sentences) why PCA is useful in this QNN setting, and what we are trading off by compressing the features.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIGmqSQliHjU"
      },
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8TYoBGFiNpL"
      },
      "source": [
        "# Task 2: Implement a QNN feature map and variational ansatz\n",
        "\n",
        "We now implement the quantum circuit building blocks of our QNN:\n",
        "\n",
        "1. A **feature map** that encodes each PCA-reduced input into `n_qubits` using angle-embedding.  \n",
        "2. A **variational ansatz** with trainable parameters $\\theta$, which will be learned during training.\n",
        "\n",
        "We will then use these building blocks inside a PennyLane QNode.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7J8ZPki3iU8J"
      },
      "outputs": [],
      "source": [
        "# Task 2: QNN building blocks (feature map + ansatz)\n",
        "\n",
        "# 1. Define a device with n_qubits wires in analytic mode (shots=None)\n",
        "# -----YOUR CODE HERE-----\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits, shots=None)\n",
        "# ---YOUR CODE ENDS HERE---\n",
        "\n",
        "def feature_map_qnn(x, scale=1.0, entangle=True):\n",
        "    \"\"\"\n",
        "    Simple angle-embedding feature map with optional entangling layer.\n",
        "\n",
        "    x: 1D array-like of length n_qubits (PCA-reduced input)\n",
        "    scale: rescaling factor for angles\n",
        "    entangle: if True, apply a CZ ring after single-qubit rotations\n",
        "    \"\"\"\n",
        "    # -----YOUR CODE HERE-----\n",
        "    # 1. Apply AngleEmbedding with rotation=\"Y\" and the given scale\n",
        "\n",
        "    # 2. If entangle=True, apply a ring of CZ or CNOT gates\n",
        "\n",
        "    # ---YOUR CODE ENDS HERE---\n",
        "    return\n",
        "\n",
        "\n",
        "def variational_ansatz(theta):\n",
        "    \"\"\"\n",
        "    Variational circuit with trainable parameters theta.\n",
        "    \n",
        "    theta: parameters with shape (number of layers, number of wires).\n",
        "\n",
        "    Structure:\n",
        "    - For each layer:\n",
        "        - Apply RY rotations on all qubits\n",
        "        - Apply a ring of CNOTs to entangle the qubits\n",
        "    \"\"\"\n",
        "    n_layers, n_wires = theta.shape\n",
        "    # -----YOUR CODE HERE-----\n",
        "    \n",
        "    # ---YOUR CODE ENDS HERE---\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 2.1 — Short answer\n",
        "Explain in a few sentences how the feature map and the variational ansatz play different roles in a QNN.  \n",
        "Relate each of them to analogous components in a classical neural network.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 2.2 — Reflection\n",
        "Why does the absence of entangling gates in a quantum feature map or variational ansatz (or any quantum circuit in general) guarantee that the the circuit's output state can be efficiently simulated classically given a classical input such as $|0\\rangle$? (2-5 sentances)\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 2.3 — Follow up\n",
        "Given this fact, can we expect any quantum advantage from a circuit composed only of single-qubit gates in a feature map or variational ansatz? Explain why or why not.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALTWnbWRiosT"
      },
      "source": [
        "# Task 3: Define the QNode and PyTorch QNN classifier\n",
        "\n",
        "We now combine the feature map and ansatz into a QNode that returns a single expectation value, and then wrap it in a PyTorch module.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Define a QNode `qnn_circuit(x, theta)`:\n",
        "   - encodes one input sample `x` using `feature_map_qnn`,\n",
        "   - applies `variational_ansatz(theta)`. Try first with scale 1 and with entangling.\n",
        "   - Return the expectation value of `PauliZ` on qubit 0. (Note that measuring PauliZ ensures that the expectation value is in [-1, 1]). Hint: Use the pennylane function `qml.expval`.\n",
        "\n",
        "2. Define a `QNNClassifier(nn.Module)` that:\n",
        "   - stores `theta` as an `nn.Parameter`,\n",
        "   - in `forward`, loops over a mini-batch of inputs,\n",
        "   - calls the QNode for each sample,\n",
        "   - maps the expectation values from [-1, 1] to probabilities in [0, 1],\n",
        "   - returns a tensor of shape `(batch_size, 1)`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 3.1 — Short answer\n",
        "Why is the expectation value of a Pauli operator (e.g. Z on one qubit) a natural scalar output for a QNN used for **binary classification**?  \n",
        "How do we convert it into a probability in [0, 1] in a good way?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50IkgM-QivyN"
      },
      "outputs": [],
      "source": [
        "# Task 3: QNode and QNN classifier with PyTorch\n",
        "\n",
        "# Decide a theta shape, e.g. for L layers and n_qubits:\n",
        "# theta_shape = (n_layers, n_qubits)\n",
        "# You may choose a different layout if you like.\n",
        "n_layers = 2\n",
        "theta_shape = (n_layers, n_qubits)\n",
        "\n",
        "@qml.qnode(dev, interface=\"torch\", diff_method=\"parameter-shift\")\n",
        "def qnn_circuit(x, theta):\n",
        "    \"\"\"\n",
        "    Quantum node for a single input sample.\n",
        "\n",
        "    x: 1D tensor with n_qubits features\n",
        "    theta: trainable parameters for the ansatz\n",
        "    \"\"\"\n",
        "    # -----YOUR CODE HERE-----\n",
        "    \n",
        "    # ---YOUR CODE ENDS HERE---\n",
        "    \n",
        "\n",
        "def test_qnn_circuit():\n",
        "    x_sample = torch.tensor([0.1, -0.2, 0.3, 0.4], dtype=torch.float32)\n",
        "    theta_sample = torch.ones(theta_shape, dtype=torch.float32)\n",
        "    ev = qnn_circuit(x_sample, theta_sample)\n",
        "    print(\"QNN circuit output (expectation value):\", ev.item())\n",
        "    assert np.isclose(ev.item(), -0.5613872, FP_TOL, FP_TOL)\n",
        "\n",
        "test_qnn_circuit()\n",
        "\n",
        "class QNNClassifier(nn.Module):\n",
        "    def __init__(self, theta_shape):\n",
        "        super().__init__()\n",
        "        # Initialize trainable parameters theta as a PyTorch Parameter\n",
        "        self.theta = nn.Parameter(torch.randn(theta_shape) * 0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: tensor of shape (batch_size, n_qubits)\n",
        "        returns: probabilities in [0,1] of shape (batch_size, 1)\n",
        "        \"\"\"\n",
        "        # -----YOUR CODE HERE-----\n",
        "        # 1. Ensure x has a batch dimension\n",
        "        # 2. For each sample in the batch, call qnn_circuit(sample, self.theta)\n",
        "        # 3. Stack expectation values into a tensor\n",
        "        # 4. Map from [-1,1] to [0,1]\n",
        "        # 5. Return p with shape (batch_size, 1)\n",
        "\n",
        "        # ---YOUR CODE ENDS HERE---\n",
        "\n",
        "\n",
        "# Instantiate model and move to Torch device\n",
        "model = QNNClassifier(theta_shape).to(device_torch)\n",
        "print(model)\n",
        "\n",
        "def test_qnn_circuit():\n",
        "    test_batch1 = torch.tensor([[0.1, -0.2, 0.3, 0.4],\n",
        "                               [0.5, 0.6, -0.7, 0.8]], dtype=torch.float32)\n",
        "    \n",
        "    test_batch2 = torch.tensor([0.0, 0.0, 0.0, 0.0], dtype=torch.float32)\n",
        "        \n",
        "    assert model.forward(test_batch1).shape == (2, 1)\n",
        "    assert model.forward(test_batch2).shape == (1, 1)\n",
        "    \n",
        "    #Assert that the output is a probability\n",
        "    p = model.forward(test_batch2).item()\n",
        "    assert 0.0 <= p <= 1.0\n",
        "\n",
        "test_qnn_circuit()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_G2PONdi9vw"
      },
      "source": [
        "# Task 4: Training the QNN\n",
        "\n",
        "We now train the QNN classifier on the PCA-reduced dataset.\n",
        "\n",
        "We will use:\n",
        "\n",
        "* `nn.BCELoss` as the loss function (binary cross-entropy on probabilities),  \n",
        "* `Adam` as the optimizer,  \n",
        "* a modest number of epochs (e.g. 15–30).\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Wrap the training data in a `DataLoader`. Shuffle the data for the training loader but not the validation loader.\n",
        "2. Implement the standard training loop:\n",
        "   - zero gradients,\n",
        "   - forward pass,\n",
        "   - compute loss,\n",
        "   - backpropagate,\n",
        "   - optimizer step.  \n",
        "3. Track and print the training loss per epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkqrqtI4iK9S"
      },
      "outputs": [],
      "source": [
        "# Create DataLoaders for training and test sets using PCA-reduced features\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = TensorDataset(\n",
        "    torch.tensor(Xtr_red, dtype=torch.double),\n",
        "    torch.tensor(y_tr, dtype=torch.double).unsqueeze(1),\n",
        ")\n",
        "val_ds = TensorDataset(\n",
        "    torch.tensor(Xval_red, dtype=torch.double),\n",
        "    torch.tensor(y_val, dtype=torch.double).unsqueeze(1),\n",
        ")\n",
        "test_ds = TensorDataset(\n",
        "    torch.tensor(Xte_red, dtype=torch.double),\n",
        "    torch.tensor(y_te, dtype=torch.double).unsqueeze(1),\n",
        ")\n",
        "# -----YOUR CODE HERE-----\n",
        "train_loader = \n",
        "val_loader = \n",
        "test_loader = \n",
        "# ---YOUR CODE ENDS HERE---\n",
        "\n",
        "print(len(train_loader), len(val_loader), len(test_loader))\n",
        "assert len(train_loader) == 8\n",
        "assert len(val_loader) == 2\n",
        "assert len(test_loader) == 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD7plbHYh67g"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n",
        "\n",
        "n_epochs = 20\n",
        "\n",
        "history = {\"loss\": [], \"val_loss\": []}\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.to(device_torch)\n",
        "        yb = yb.to(device_torch)\n",
        "\n",
        "        # -----YOUR CODE HERE-----\n",
        "\n",
        "        # ---YOUR CODE ENDS HERE---\n",
        "\n",
        "        epoch_loss += loss.item() * xb.size(0)\n",
        "    epoch_loss /= len(train_loader.dataset)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            val_loss += loss.item() * xb.size(0)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "\n",
        "    history[\"loss\"].append(epoch_loss)\n",
        "    history[\"val_loss\"].append(val_loss)\n",
        "    print(f\"Epoch {epoch:02d}/{n_epochs} - loss: {epoch_loss:.4f} - val_loss: {val_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VX4V-XnjK_D"
      },
      "outputs": [],
      "source": [
        "# Plot training loss\n",
        "\n",
        "plt.figure(figsize=(4, 3))\n",
        "plt.plot(history[\"loss\"], marker='o')\n",
        "plt.plot(history[\"val_loss\"], marker='o')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Train loss (BCE)\")\n",
        "plt.title(\"QNN training loss\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T04N9kf0jMBw"
      },
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 4.1 — Short answer\n",
        "Comment briefly on the training and validation curves you obtained.\n",
        "Does the loss decrease steadily? Do you see signs of underfitting or overfitting?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOMFzliajR7k"
      },
      "source": [
        "# Task 5: Evaluation on the test set\n",
        "\n",
        "Finally, we evaluate the trained QNN on the **held-out test set**.\n",
        "\n",
        "1. Compute predicted probabilities on the test set.  \n",
        "2. Convert probabilities to class predictions using a 0.5 threshold.  \n",
        "3. Compute and print:\n",
        "   - test accuracy,  \n",
        "   - confusion matrix,  \n",
        "   - classification report (optional).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mjo4xZ_3jX1m"
      },
      "outputs": [],
      "source": [
        "# Evaluation on test set\n",
        "\n",
        "model.eval()\n",
        "all_probs = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.to(device_torch)\n",
        "        yb = yb.to(device_torch)\n",
        "\n",
        "        probs = model(xb)  # (batch_size, 1)\n",
        "        all_probs.append(probs.cpu())\n",
        "        all_labels.append(yb.cpu())\n",
        "\n",
        "all_probs = torch.cat(all_probs, dim=0)\n",
        "all_labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "# Convert probabilities to class predictions\n",
        "y_pred = (all_probs >= 0.5).int()\n",
        "y_true = all_labels.int()\n",
        "\n",
        "acc = accuracy_score(y_true.numpy(), y_pred.numpy())\n",
        "cm = confusion_matrix(y_true.numpy(), y_pred.numpy())\n",
        "\n",
        "print(f\"Test accuracy: {acc:.3f}\")\n",
        "print(\"Confusion matrix [[TN, FP], [FN, TP]]:\")\n",
        "print(cm)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.show()\n",
        "\n",
        "print(classification_report(y_true.numpy(), y_pred.numpy()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8grB5epjaMx"
      },
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question 5.1 — Short answer\n",
        "Does the confusion matrix show any interesting asymmetry (e.g., more false positives than false negatives)?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNltSHpYjeEN"
      },
      "source": [
        "# Bonus: Gradient inspection and barren plateaus\n",
        "\n",
        "In this bonus exercise, you will perform a more detailed gradient inspection on your QNN to connect with the concept of barren plateaus.\n",
        "\n",
        "We will focus on three aspects:\n",
        "\n",
        "1. Gradient norms at initialization for different circuit depths (`n_layers`).  \n",
        "2. Gradient norms during training (how they evolve across epochs).  \n",
        "3. Distribution of per-parameter gradients at a given point in training.\n",
        "\n",
        "\n",
        "You may reuse your QNN architecture from the main tasks (feature map, ansatz, QNode, and `QNNClassifier`), or define a simplified variant dedicated to this analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30UuPJADkctx"
      },
      "source": [
        "## Bonus 0 — Helper: single-batch gradient computation\n",
        "\n",
        "We start by writing a helper function that:\n",
        "\n",
        "- Takes a model, a data loader, and a loss function.  \n",
        "- Grabs one mini-batch from the loader.  \n",
        "- Computes the loss and then the gradient of the loss w.r.t. `model.theta`.  \n",
        "- Returns the L2 norm of that gradient, divided by the square root of the number of parameters (and optionally the gradient tensor itself).\n",
        "\n",
        "We will reuse this helper in the following subtasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqPqS0P_kmpj"
      },
      "outputs": [],
      "source": [
        "# Bonus A.0 — Helper: compute gradient norm for one batch\n",
        "\n",
        "# Normalized gradient norm computation\n",
        "def gradient_norm_for_one_batch(model, data_loader, criterion, device=device_torch):\n",
        "    \"\"\"\n",
        "    Compute the L2 norm of the gradient of the loss w.r.t. model.theta\n",
        "    using a single batch from data_loader.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    # Take the first batch only\n",
        "    xb, yb = next(iter(data_loader))\n",
        "    xb = xb.to(device)\n",
        "    yb = yb.to(device)\n",
        "\n",
        "    # Zero existing gradients\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    preds = model(xb)\n",
        "    loss = criterion(preds, yb)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Extract gradient of theta\n",
        "    grad_theta = model.theta.grad  # tensor with same shape as theta\n",
        "    grad_norm = torch.norm(grad_theta).item() / math.sqrt(grad_theta.numel())\n",
        "\n",
        "    return grad_norm, grad_theta.detach().cpu(), loss.item()\n",
        "\n",
        "# Quick sanity check (optional, only if model is already defined)\n",
        "criterion = nn.BCELoss()\n",
        "g_norm, g_tensor, loss_val = gradient_norm_for_one_batch(model, train_loader, criterion)\n",
        "print(\"Gradient norm (one batch):\", g_norm, \"Loss:\", loss_val)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r_5JmzskrGq"
      },
      "source": [
        "## Bonus 1 — Gradient norms at initialization vs circuit depth\n",
        "\n",
        "In this part, you will study how the gradient norm at random initialization depends on the circuit depth `n_layers`.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Choose a list of depths, e.g. `depth_list = [1, 2, 3]` (you may add more if runtime allows).  \n",
        "2. For each depth:\n",
        "   - Rebuild a fresh QNN (same architecture as in the main task, but with that `n_layers`).  \n",
        "   - Randomly initialize the parameters (the default PyTorch initialization is fine).  \n",
        "   - Compute the gradient norm on a single training batch using `gradient_norm_for_one_batch`.  \n",
        "3. Store the gradient norms and print them (and optionally plot them).  \n",
        "4. Comment on whether deeper circuits tend to show smaller gradient norms at initialization.\n",
        "\n",
        "You can reuse your `QNNClassifier` class, but you will need to make `n_layers` configurable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIsE0jI1kze5"
      },
      "outputs": [],
      "source": [
        "# Bonus A.1 — Gradient norm vs depth at initialization\n",
        "\n",
        "# Make sure you have a way to construct a QNNClassifier with a given n_layers.\n",
        "# For example, you can wrap the model creation in a small factory function.\n",
        "\n",
        "def make_qnn_model(n_layers, n_qubits):\n",
        "    \"\"\"\n",
        "    Factory for QNNClassifier with given depth n_layers and n_qubits.\n",
        "    Adjust theta_shape and ansatz accordingly.\n",
        "    \"\"\"\n",
        "    # Example: theta_shape = (n_layers, n_qubits)\n",
        "    theta_shape = (n_layers, n_qubits)\n",
        "    model = QNNClassifier(theta_shape).to(device_torch)\n",
        "    return model\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "depth_list = [1, 2, 3, 4, 5, 6, 7]  # you can add 4, 5, ... if time allows\n",
        "grad_norms_depth = []\n",
        "\n",
        "for depth in depth_list:\n",
        "# -----YOUR CODE HERE-----\n",
        "\n",
        "# ---YOUR CODE ENDS HERE---\n",
        "\n",
        "# Optional: plot gradient norms vs depth\n",
        "plt.figure(figsize=(4, 3))\n",
        "plt.plot(depth_list, grad_norms_depth, marker=\"o\")\n",
        "plt.xlabel(\"n_layers (circuit depth)\")\n",
        "plt.ylabel(\"Gradient norm at init\")\n",
        "plt.title(\"Gradient norm vs depth at initialization\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yjx5yAkbk2SE"
      },
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question Bonus 1.1 — Interpretation\n",
        "Comment on the gradient norms vs depth that you observed.\n",
        "\n",
        "- Do deeper circuits tend to show smaller gradient norms at random initialization?  \n",
        "- How does this relate qualitatively to the idea of barren plateaus (even in this small setup)?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fcwwuygk-pg"
      },
      "source": [
        "## Bonus 2 — Gradient norm across training epochs\n",
        "\n",
        "We now study how the gradient norm evolves during training for a fixed circuit depth.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Pick one depth (e.g. `n_layers = 2`).  \n",
        "2. Train the QNN for a small number of epochs (e.g. 10) as in the main assignment.  \n",
        "3. At each epoch:\n",
        "   - After the weight update, compute the gradient norm on one batch using `gradient_norm_for_one_batch`.  \n",
        "   - Store this value in a list `grad_norms_epochs`.  \n",
        "4. Plot gradient norm vs epoch, and compare this with the training loss curve.  \n",
        "5. Comment on whether gradients tend to shrink as training progresses.\n",
        "\n",
        "> Hint: You can reuse your training loop, but add a call to `gradient_norm_for_one_batch` at the end of each epoch, using the current model parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwGcF7JDlH8G"
      },
      "outputs": [],
      "source": [
        "# Bonus A.2 — Gradient norm vs epoch for a fixed depth\n",
        "\n",
        "# Choose a depth\n",
        "depth_for_training = 2\n",
        "model_train = make_qnn_model(depth_for_training, n_qubits)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model_train.parameters(), lr=0.02)\n",
        "\n",
        "n_epochs_bonus = 10\n",
        "losses_epochs = []\n",
        "grad_norms_epochs = []\n",
        "\n",
        "for epoch in range(1, n_epochs_bonus + 1):\n",
        "    model_train.train()\n",
        "    epoch_loss = 0.0\n",
        "    # -----YOUR CODE HERE-----\n",
        "\n",
        "    # ---YOUR CODE ENDS HERE---\n",
        "    grad_norms_epochs.append(g_norm)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d}/{n_epochs_bonus} - Loss: {epoch_loss:.4f}, Grad norm: {g_norm:.4e}\")\n",
        "\n",
        "# Plot loss and gradient norm vs epoch\n",
        "fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "ax1.set_xlabel(\"Epoch\")\n",
        "ax1.set_ylabel(\"Train loss (BCE)\", color=\"C0\")\n",
        "ax1.plot(range(1, n_epochs_bonus+1), losses_epochs, marker=\"o\", color=\"C0\", label=\"Loss\")\n",
        "ax1.tick_params(axis=\"y\", labelcolor=\"C0\")\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel(\"Gradient norm\", color=\"C1\")\n",
        "ax2.plot(range(1, n_epochs_bonus+1), grad_norms_epochs, marker=\"s\", color=\"C1\", label=\"Grad norm\")\n",
        "ax2.tick_params(axis=\"y\", labelcolor=\"C1\")\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.title(f\"Depth = {depth_for_training}: Loss and gradient norm vs epoch\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOXamdZPlMAg"
      },
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question Bonus 2.1 — Observation\n",
        "Describe how the gradient norm changes during training: Does it decrease, stay roughly constant, or fluctuate?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question Bonus 2.2 — Interpretation\n",
        "How does this relate to the changes in the training loss?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question Bonus 2.3 — Interpretation\n",
        "In general, if our QNN suffered from a barren plateau, how would that affect the gradient norm?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWswq1DflVCs"
      },
      "source": [
        "## Bonus 3 — Distribution of per-parameter gradients\n",
        "\n",
        "So far, we have focused on the normalized L2 norm of the gradient.  \n",
        "Now, we take a closer look at the distribution of individual gradient components.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Using a model (for some depth), compute the gradient of the loss w.r.t. `model.theta` on one batch.  \n",
        "2. Extract all gradient entries as a 1D array and plot a histogram of their values.  \n",
        "Do step 1 and 2:\n",
        "   - once at initialization, and  \n",
        "   - once after training, and compare the two histograms.\n",
        "\n",
        "This gives a sense of whether gradients are broadly distributed or tend to concentrate tightly around zero.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbWo_z4BlhCM"
      },
      "outputs": [],
      "source": [
        "# Bonus A.3 — Gradient distribution (histogram)\n",
        "\n",
        "def gradient_histogram(model, data_loader, criterion, device=device_torch, title=\"\"):\n",
        "    \"\"\"\n",
        "    Compute gradients on one batch and plot a histogram of gradient values.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    xb, yb = next(iter(data_loader))\n",
        "    xb = xb.to(device)\n",
        "    yb = yb.to(device)\n",
        "    \n",
        "    # -----YOUR CODE HERE-----\n",
        "\n",
        "    # ---YOUR CODE ENDS HERE---\n",
        "\n",
        "    grad_vals = model.theta.grad.detach().cpu().numpy().ravel()\n",
        "\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    plt.hist(grad_vals, bins=40, alpha=0.8)\n",
        "    plt.xlabel(\"Gradient value\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.title(title)\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"{title} — mean grad: {grad_vals.mean():.2e}, std grad: {grad_vals.std():.2e}\")\n",
        "    return grad_vals\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "depth = 2\n",
        "# Example: histogram at initialization for specific depth\n",
        "model_init = make_qnn_model(depth, n_qubits)\n",
        "print(\"Gradient distribution at initialization (depth=2):\")\n",
        "_ = gradient_histogram(model_init, train_loader, criterion, device=device_torch,\n",
        "                       title=\"Gradients at init (depth=2)\")\n",
        "\n",
        "# Example: histogram after training for depth=2 (reuse model_train from Bonus A.2 if available)\n",
        "try:\n",
        "    print(\"Gradient distribution after training (depth=2):\")\n",
        "    _ = gradient_histogram(model_train, train_loader, criterion, device=device_torch,\n",
        "                           title=\"Gradients after training (depth=2)\")\n",
        "except NameError:\n",
        "    print(\"model_train not found. Run Bonus A.2 first or train a model for this analysis.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare the gradient histograms you obtained:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB-ifGkolmQ1"
      },
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question Bonus 3.1 — Observation\n",
        "Are gradients broadly distributed or sharply concentrated near zero?  \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAcEJ8k3jJep"
      },
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question Bonus 3.2 — Short answer\n",
        "Does training make them more or less concentrated?  \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Question Bonus 3.3 — Reflection\n",
        "How would a highly concentrated distribution of very small gradients relate to barren plateaus?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feedback to us"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"max-width: 1200px; margin: auto; border: 1px solid #004791; border-left: 6px solid #004791; border-radius: 6px; padding: 0.8em 1em; background-color: #1b1b1bff; color: #eee;\">\n",
        "\n",
        "### Optional question\n",
        "Was there any part of the tasks where you struggled for some \"unnecessary\" reason? (Errors in the notebook, bad instructions etc.)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer**: [Your optional answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Disclosure of AI Usage (Mandatory)\n",
        "Fill in this part disclosing any AI usage before submitting the assignment by describing your use of LLMs or other AI-based tools in this assignment.\n",
        "\n",
        "For each task, we ask you to provide information about:\n",
        "- **Tools/models used**.\n",
        "- **Per‑task usage**: for each task, a brief summary of what the tool was used for.\n",
        "- **Prompts/transcripts**: main prompts or a summary of interactions (a link or screenshot is acceptable if long).\n",
        "- **Validation**: how you checked and verified the correctness of AI-generated outputs (tests run, docs consulted, comparisons, plots etc.).\n",
        "\n",
        "Disclosure:\n",
        "- **Task 0**: [...describe your use...]\n",
        "\n",
        "- **Task 1**: [...describe your use...]\n",
        "\n",
        "- **Task 2**: [...describe your use...]\n",
        "\n",
        "- **Task 3**: [...describe your use...]\n",
        "\n",
        "- **Task 4**: [...describe your use...]\n",
        "\n",
        "- **Task 5**: [...describe your use...]\n",
        "\n",
        "- **Bonus 1**: [...describe your use...]\n",
        "\n",
        "- **Bonus 2**: [...describe your use...]\n",
        "\n",
        "- **Bonus 3**: [...describe your use...]\n",
        "\n",
        "If you did not use any AI tools for a given exercise, specify this by writing \"None\". If you did not complete the bonus exercises, you can leave those fields empty."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "DD2367",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
